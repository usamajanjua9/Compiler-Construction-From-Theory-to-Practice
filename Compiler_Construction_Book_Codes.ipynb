{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lexical Analyzer"
      ],
      "metadata": {
        "id": "13kIk0UfTZ50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Define Token Patterns\n",
        "First, define the set of tokens that your language supports along with their corresponding regular expressions. For simplicity, let’s consider a small language that includes identifiers, integers, parentheses, and basic arithmetic operators."
      ],
      "metadata": {
        "id": "zcVz2tZpTtTY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVOLW-X3TSi9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define the token categories and their regex patterns\n",
        "TOKEN_PATTERNS = {\n",
        "    'INTEGER': r'\\d+',\n",
        "    'IDENTIFIER': r'[a-zA-Z_][a-zA-Z_0-9]*',\n",
        "    'OPERATOR': r'[\\+\\-\\*/]',\n",
        "    'LPAREN': r'\\(',\n",
        "    'RPAREN': r'\\)',\n",
        "    'WHITESPACE': r'\\s+',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Build the Lexer\n",
        "The lexer reads the input text, matches the text against the defined patterns, and produces a list of tokens."
      ],
      "metadata": {
        "id": "ph4uyDrWTvZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Token:\n",
        "    def __init__(self, type, value):\n",
        "        self.type = type\n",
        "        self.value = value\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'Token({self.type}, {self.value})'\n",
        "\n",
        "class Lexer:\n",
        "    def __init__(self, patterns):\n",
        "        # Compile the string patterns into regex objects for faster matching\n",
        "        self.patterns = {key: re.compile(pattern) for key, pattern in patterns.items()}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        pos = 0\n",
        "        tokens = []\n",
        "\n",
        "        while pos < len(text):\n",
        "            match = None\n",
        "            for type, pattern in self.patterns.items():\n",
        "                match = pattern.match(text, pos)\n",
        "                if match:\n",
        "                    # Skip whitespace\n",
        "                    if type != 'WHITESPACE':\n",
        "                        value = match.group(0)\n",
        "                        tokens.append(Token(type, value))\n",
        "                    pos = match.end()\n",
        "                    break\n",
        "            if not match:\n",
        "                raise SyntaxError(f'Illegal character at index {pos}')\n",
        "        return tokens\n"
      ],
      "metadata": {
        "id": "GF-3ymU6ToIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Test the Lexer\n",
        "Now, test the lexer with a simple input string to see if it correctly identifies the tokens."
      ],
      "metadata": {
        "id": "fGtEzCFCTYhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    input_text = \"(sum + 47) / total\"\n",
        "    lexer = Lexer(TOKEN_PATTERNS)\n",
        "    tokens = lexer.tokenize(input_text)\n",
        "\n",
        "    for token in tokens:\n",
        "        print(token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI_AgqKWT4dt",
        "outputId": "7f08e6cc-b08f-42ca-b27b-228aded30871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token(LPAREN, ()\n",
            "Token(IDENTIFIER, sum)\n",
            "Token(OPERATOR, +)\n",
            "Token(INTEGER, 47)\n",
            "Token(RPAREN, ))\n",
            "Token(OPERATOR, /)\n",
            "Token(IDENTIFIER, total)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "Define Token Patterns: Regular expressions for each token type are defined.\n",
        "Token and Lexer Classes: A Token class to represent individual tokens and a Lexer class to tokenize the input string.\n",
        "The Lexer class’s tokenize method processes the input text, matches each part of the string to the defined token patterns, and generates a list of Token objects.\n",
        "Skipping Whitespace: Whitespace tokens are recognized but not added to the final list of tokens.\n",
        "Error Handling: If the lexer encounters an unrecognized character, it raises a SyntaxError.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aQIn71U4T9zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsers (Syntax Analysis): Run on Spyder/Pycharm etc"
      ],
      "metadata": {
        "id": "P1FFcqYiKxRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install PLY\n",
        "Ensure you have PLY installed in your environment:"
      ],
      "metadata": {
        "id": "MAlexFLoUL0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ply\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWbxrWFNCmmY",
        "outputId": "9ec0777a-e28f-4277-edef-e815d373d176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define the Lexer\n",
        "The lexer breaks the input text into a stream of tokens. We define tokens for numbers, addition and multiplication operators, parentheses, and whitespace."
      ],
      "metadata": {
        "id": "87KM4uc3Dy0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "# List of token names.\n",
        "tokens = (\n",
        "    'NUMBER',\n",
        "    'PLUS',\n",
        "    'TIMES',\n",
        "    'LPAREN',\n",
        "    'RPAREN',\n",
        ")\n",
        "\n",
        "# Regular expression rules for simple tokens\n",
        "t_PLUS = r'\\+'\n",
        "t_TIMES = r'\\*'\n",
        "t_LPAREN = r'\\('\n",
        "t_RPAREN = r'\\)'\n",
        "\n",
        "# A regular expression rule with some action code\n",
        "def t_NUMBER(t):\n",
        "    r'\\d+'\n",
        "    t.value = int(t.value)    # Convert string to integer\n",
        "    return t\n",
        "\n",
        "# Define a rule so we can track line numbers\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)\n",
        "\n",
        "# A string containing ignored characters (spaces and tabs)\n",
        "t_ignore  = ' \\t'\n",
        "\n",
        "# Error handling rule\n",
        "def t_error(t):\n",
        "    print(f\"Illegal character '{t.value[0]}'\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "# Build the lexer\n",
        "lexer = lex.lex()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "_-1ugqHlDza_",
        "outputId": "bbffe732-b911-4e83-d49e-f21072959947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<module '__main__'> is a built-in module",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a85febc6df35>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Build the lexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mlexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mlex\u001b[0;34m(module, object, debug, optimize, lextab, reflags, nowarn, outputdir, debuglog, errorlog)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0mlinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't build lexer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mvalidate_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_literals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mvalidate_rules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;31m# -----------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mvalidate_module\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m   1120\u001b[0m     \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    938\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;31m# Invalidate cache if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is a built-in module'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__module__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <module '__main__'> is a built-in module"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the Parser\n",
        "The parser analyzes the token stream to determine its grammatical structure with respect to the given grammar."
      ],
      "metadata": {
        "id": "BVVQMOjsMBB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ply.yacc as yacc\n",
        "\n",
        "# Define the precedence and associativity of operators\n",
        "precedence = (\n",
        "    ('left', 'PLUS'),\n",
        "    ('left', 'TIMES'),\n",
        ")\n",
        "\n",
        "# The grammar rules and handler functions\n",
        "def p_expression_plus(p):\n",
        "    'expression : expression PLUS term'\n",
        "    p[0] = p[1] + p[3]  # Perform addition\n",
        "\n",
        "def p_expression_times(p):\n",
        "    'expression : term TIMES factor'\n",
        "    p[0] = p[1] * p[3]  # Perform multiplication\n",
        "\n",
        "def p_expression_term(p):\n",
        "    'expression : term'\n",
        "    p[0] = p[1]\n",
        "\n",
        "def p_term_factor(p):\n",
        "    'term : factor'\n",
        "    p[0] = p[1]\n",
        "\n",
        "def p_factor_number(p):\n",
        "    'factor : NUMBER'\n",
        "    p[0] = p[1]\n",
        "\n",
        "def p_factor_expr(p):\n",
        "    'factor : LPAREN expression RPAREN'\n",
        "    p[0] = p[2]  # Parentheses are for grouping, so just return the inner expression value\n",
        "\n",
        "# Error rule for syntax errors\n",
        "def p_error(p):\n",
        "    print(\"Syntax error in input!\")\n",
        "\n",
        "# Build the parser\n",
        "parser = yacc.yacc()"
      ],
      "metadata": {
        "id": "55Jv-luRKsbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Using the Parser\n",
        "Now, you can use the lexer and parser to evaluate expressions."
      ],
      "metadata": {
        "id": "1YK2nPtVMF5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it out\n",
        "data = \"3 * 4 + 5\"\n",
        "\n",
        "# Give the lexer some input\n",
        "lexer.input(data)\n",
        "# Iterate through tokens\n",
        "for token in lexer:\n",
        "    print(token)\n",
        "# Parse the input string\n",
        "result = parser.parse(data)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "hEeJp2E9MGkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Tokens: We define tokens for numbers, plus, times, left parenthesis, and right parenthesis.\n",
        "Lexer Rules: Each token type has an associated regular expression. For numbers, we also convert the matched string to an integer.\n",
        "Parser Rules: The parser rules define how tokens can be combined to form valid expressions. We use Python functions for each grammar rule. The p[0] = p[1] + p[3] syntax is used to calculate the result of binary operations, where p[1] and p[3] refer to the operands and p[2] is the operator (ignored in calculation).\n",
        "Precedence: The precedence rules ensure that multiplication is evaluated before addition.\n",
        "Error Handling: Basic error handling is provided to skip illegal characters in the lexer and to print a message in case of syntax errors in the parser.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ke2MqzZMMBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Analysis (Manual)"
      ],
      "metadata": {
        "id": "n84C_i-uMPtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Define Symbol Table\n",
        "A symbol table is needed to store information about variables (like their types and scopes). Here's a basic implementation:"
      ],
      "metadata": {
        "id": "iMg7YBU1NLmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Symbol:\n",
        "    def __init__(self, name, type=None):\n",
        "        self.name = name\n",
        "        self.type = type\n",
        "\n",
        "class SymbolTable:\n",
        "    def __init__(self, parent=None):\n",
        "        self.symbols = {}\n",
        "        self.parent = parent\n",
        "\n",
        "    def add(self, symbol):\n",
        "        self.symbols[symbol.name] = symbol\n",
        "\n",
        "    def get(self, name):\n",
        "        symbol = self.symbols.get(name, None)\n",
        "        if symbol is not None:\n",
        "            return symbol\n",
        "        if self.parent is not None:\n",
        "            return self.parent.get(name)\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "5vip2wy-SHoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Enhance the Parser for Semantic Analysis\n",
        "Extend the parser to perform semantic analysis, such as type checking and symbol table management. This example builds upon a hypothetical parser that produces an AST."
      ],
      "metadata": {
        "id": "xkXEKqMLSKov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    pass\n",
        "\n",
        "class BinOp(Node):\n",
        "    def __init__(self, left, op, right):\n",
        "        self.left = left\n",
        "        self.op = op\n",
        "        self.right = right\n",
        "\n",
        "class Num(Node):\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.type = 'int' if isinstance(value, int) else 'float'\n",
        "\n",
        "class Var(Node):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.type = None  # To be filled in by the semantic analyzer\n",
        "\n",
        "# Assume `ast` is the root of the AST produced by the parser\n"
      ],
      "metadata": {
        "id": "sA-HLtyESLED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Implement the Semantic Analyzer\n",
        "The semantic analyzer walks the AST, updates the symbol table, and performs type checking."
      ],
      "metadata": {
        "id": "3te14QU3SRBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.global_scope = SymbolTable()\n",
        "\n",
        "    def visit(self, node, table):\n",
        "        method_name = f'visit_{type(node).__name__}'\n",
        "        visitor = getattr(self, method_name, self.generic_visit)\n",
        "        return visitor(node, table)\n",
        "\n",
        "    def visit_BinOp(self, node, table):\n",
        "        left_type = self.visit(node.left, table)\n",
        "        right_type = self.visit(node.right, table)\n",
        "        if left_type != right_type:\n",
        "            raise TypeError(\"Type mismatch in binary operation\")\n",
        "        return left_type  # Assuming the operation doesn't change the type\n",
        "\n",
        "    def visit_Num(self, node, table):\n",
        "        return node.type\n",
        "\n",
        "    def visit_Var(self, node, table):\n",
        "        symbol = table.get(node.name)\n",
        "        if symbol is None:\n",
        "            raise NameError(f\"Variable '{node.name}' not defined\")\n",
        "        node.type = symbol.type\n",
        "        return node.type\n",
        "\n",
        "    def generic_visit(self, node, table):\n",
        "        raise Exception(f\"No visit_{type(node).__name__} method\")\n",
        "\n",
        "    def analyze(self, ast):\n",
        "        self.visit(ast, self.global_scope)\n"
      ],
      "metadata": {
        "id": "e-nMqdxASRps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Use the Semantic Analyzer\n",
        "After parsing the AST, pass it to the semantic analyzer for analysis."
      ],
      "metadata": {
        "id": "SBtnx3TbSVcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `ast` is the root node of the AST\n",
        "analyzer = SemanticAnalyzer()\n",
        "try:\n",
        "    analyzer.analyze(ast)\n",
        "    print(\"Semantic analysis completed successfully.\")\n",
        "except (TypeError, NameError) as e:\n",
        "    print(f\"Semantic error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCFwUWnzSV-9",
        "outputId": "aa31cb54-fcb6-4f4f-ae91-bc80a87c2792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic error: Variable 'a' not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate different outputs from the semantic analyzer based on various inputs, let's consider a few scenarios in our hypothetical language environment. These scenarios will illustrate successful semantic analysis and common semantic errors such as type mismatches and undeclared variables.\n",
        "\n",
        "Prerequisites\n",
        "Assuming the semantic analyzer and all necessary classes (SymbolTable, Symbol, Node, BinOp, Num, Var, etc.) have been defined as previously described, we'll create some example ASTs that represent different expressions or statements in the language.\n",
        "\n",
        "Scenario 1: Successful Semantic Analysis\n",
        "Input Code: 3 + 4\n",
        "\n",
        "This code should pass semantic analysis since both operands are integers, and the operation is valid."
      ],
      "metadata": {
        "id": "O6nk4QMqSfRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AST for \"3 + 4\"\n",
        "ast = BinOp(Num(3), '+', Num(4))\n",
        "\n",
        "# Semantic analysis\n",
        "analyzer = SemanticAnalyzer()\n",
        "try:\n",
        "    analyzer.analyze(ast)\n",
        "    print(\"Semantic analysis completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Semantic error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y-whQBiSgCd",
        "outputId": "49bfe2de-01f0-4c48-ba0d-9b62c55cad41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic analysis completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 2: Type Mismatch Error\n",
        "Input Code: 3 + 4.5\n",
        "\n",
        "This code should raise a type mismatch error during semantic analysis because the operands are of different types."
      ],
      "metadata": {
        "id": "JTFR81isSkUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AST for \"3 + 4.5\"\n",
        "ast = BinOp(Num(3), '+', Num(4.5))\n",
        "\n",
        "# Semantic analysis\n",
        "analyzer = SemanticAnalyzer()\n",
        "try:\n",
        "    analyzer.analyze(ast)\n",
        "    print(\"Semantic analysis completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Semantic error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iuzkNKESkzF",
        "outputId": "03d7a216-d9cc-436a-87cc-4b99df0d674c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic error: Type mismatch in binary operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 3: Undeclared Variable Error\n",
        "Input Code: a + 5 (assuming a has not been declared)\n",
        "\n",
        "This code should raise an undeclared variable error since a has not been added to the symbol table."
      ],
      "metadata": {
        "id": "c9Y_YGSkSpw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AST for \"a + 5\", assuming 'a' is not declared\n",
        "ast = BinOp(Var('a'), '+', Num(5))\n",
        "\n",
        "# Semantic analysis\n",
        "analyzer = SemanticAnalyzer()\n",
        "try:\n",
        "    analyzer.analyze(ast)\n",
        "    print(\"Semantic analysis completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Semantic error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBisYcdjSqQr",
        "outputId": "270f36dd-053a-44a6-b067-12dc84b698ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic error: Variable 'a' not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete compiler"
      ],
      "metadata": {
        "id": "QtaA0_GuWeBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install PLY\n",
        "Ensure PLY is installed in your environment:"
      ],
      "metadata": {
        "id": "ytIP8CfJWiag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ply\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FbjptnSWmWj",
        "outputId": "b3f9dec1-4b4d-4f05-f2c8-12b2ba57d31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define the Lexer\n",
        "First, define the lexer using PLY's lexing capabilities. This lexer recognizes tokens for identifiers, numbers, assignment operators, arithmetic operators, and parentheses."
      ],
      "metadata": {
        "id": "itbuoSg2Wn1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "tokens = ('NUMBER', 'ID', 'ASSIGN', 'PLUS', 'TIMES', 'LPAREN', 'RPAREN')\n",
        "\n",
        "t_ASSIGN = r'='\n",
        "t_PLUS = r'\\+'\n",
        "t_TIMES = r'\\*'\n",
        "t_LPAREN = r'\\('\n",
        "t_RPAREN = r'\\)'\n",
        "\n",
        "def t_ID(t):\n",
        "    r'[a-zA-Z_][a-zA-Z_0-9]*'\n",
        "    return t\n",
        "\n",
        "def t_NUMBER(t):\n",
        "    r'\\d+'\n",
        "    t.value = int(t.value)\n",
        "    return t\n",
        "\n",
        "t_ignore = ' \\t\\n'\n",
        "\n",
        "def t_error(t):\n",
        "    print(f\"Illegal character '{t.value[0]}'\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "lexer = lex.lex()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "IhczqTwyWp8F",
        "outputId": "db037430-34af-4a46-c87d-25be922b696e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<module '__main__'> is a built-in module",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-8b2678be9887>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mlexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mlex\u001b[0;34m(module, object, debug, optimize, lextab, reflags, nowarn, outputdir, debuglog, errorlog)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0mlinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't build lexer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mvalidate_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_literals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mvalidate_rules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;31m# -----------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ply/lex.py\u001b[0m in \u001b[0;36mvalidate_module\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m   1120\u001b[0m     \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    938\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;31m# Invalidate cache if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is a built-in module'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__module__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <module '__main__'> is a built-in module"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the Symbol Table\n",
        "Implement a simple symbol table to track variable declarations."
      ],
      "metadata": {
        "id": "gsJ3LiKoWr0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SymbolTable:\n",
        "    def __init__(self):\n",
        "        self.table = {}\n",
        "\n",
        "    def define(self, name, type):\n",
        "        self.table[name] = type\n",
        "\n",
        "    def lookup(self, name):\n",
        "        return self.table.get(name, None)\n"
      ],
      "metadata": {
        "id": "SK2EBjbjWuAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Define the Parser and Integrate Semantic Analysis\n",
        "Define the parser using PLY's yacc module, integrating semantic checks using the symbol table."
      ],
      "metadata": {
        "id": "PjrFWUINWvhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ply.yacc as yacc\n",
        "\n",
        "# Assuming the lexer tokens are defined as above\n",
        "\n",
        "precedence = (\n",
        "    ('left', 'PLUS'),\n",
        "    ('left', 'TIMES'),\n",
        ")\n",
        "\n",
        "# Symbol table for semantic analysis\n",
        "symtable = SymbolTable()\n",
        "\n",
        "def p_statement_assign(p):\n",
        "    'statement : ID ASSIGN expression'\n",
        "    # Semantic check: Ensure variable is defined\n",
        "    if symtable.lookup(p[1]) is None:\n",
        "        print(f\"Semantic error: Undefined variable '{p[1]}' at assignment\")\n",
        "    else:\n",
        "        # Here, we simply print the assignment for demonstration\n",
        "        print(f\"Assigning: {p[1]} = {p[3]}\")\n",
        "\n",
        "def p_statement_decl(p):\n",
        "    'statement : ID'\n",
        "    # Assume every ID token is a variable declaration for simplicity\n",
        "    symtable.define(p[1], 'int')  # Simulate integer type\n",
        "    print(f\"Declared variable: {p[1]}\")\n",
        "\n",
        "def p_expression_binop(p):\n",
        "    '''expression : expression PLUS expression\n",
        "                  | expression TIMES expression'''\n",
        "    if p[2] == '+':\n",
        "        p[0] = p[1] + p[3]\n",
        "    elif p[2] == '*':\n",
        "        p[0] = p[1] * p[3]\n",
        "\n",
        "def p_expression_group(p):\n",
        "    'expression : LPAREN expression RPAREN'\n",
        "    p[0] = p[2]\n",
        "\n",
        "def p_expression_number(p):\n",
        "    'expression : NUMBER'\n",
        "    p[0] = p[1]\n",
        "\n",
        "def p_expression_id(p):\n",
        "    'expression : ID'\n",
        "    # Semantic check: Ensure variable is defined\n",
        "    if symtable.lookup(p[1]) is None:\n",
        "        print(f\"Semantic error: Undefined variable '{p[1]}'\")\n",
        "    else:\n",
        "        # Placeholder for a variable's value\n",
        "        p[0] = f\"value_of({p[1]})\"\n",
        "\n",
        "def p_error(p):\n",
        "    print(\"Syntax error in input!\")\n",
        "\n",
        "parser = yacc.yacc()\n"
      ],
      "metadata": {
        "id": "UQuaiiLGWxSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Test the Compiler Components\n",
        "Now, test the lexer, parser, and semantic analysis with a simple input."
      ],
      "metadata": {
        "id": "yugMxuhmWzLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = \"\"\"\n",
        "x = 3\n",
        "y = x + 4\n",
        "z = y * 2\n",
        "\"\"\"\n",
        "\n",
        "# Test Lexer\n",
        "print(\"Lexing result:\")\n",
        "lexer.input(test_input)\n",
        "for tok in lexer:\n",
        "    print(tok)\n",
        "\n",
        "# Test Parser and Semantic Analysis\n",
        "print(\"\\nParsing and Semantic Analysis result:\")\n",
        "parser.parse(test_input)\n"
      ],
      "metadata": {
        "id": "we1pGSu9W1ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Output\n",
        "The lexing step should output the tokens found in the test_input. Then, the parsing and semantic analysis steps should print messages about variable declarations, assignments, and any semantic errors detected (such as using an undefined variable)."
      ],
      "metadata": {
        "id": "O7Kai5zcW6xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MINI Compiler All Steps"
      ],
      "metadata": {
        "id": "BcZS6Dgutx4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Token specification\n",
        "TOKEN_SPECIFICATION = [\n",
        "    ('NUMBER',   r'\\d+'),\n",
        "    ('OPERATOR', r'[+\\-*/]'),\n",
        "    ('LPAREN',   r'\\('),\n",
        "    ('RPAREN',   r'\\)'),\n",
        "    ('WS',       r'\\s+'),\n",
        "]\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize(expression):\n",
        "    token_regex = '|'.join(f'(?P<{name}>{pattern})' for name, pattern, *_ in TOKEN_SPECIFICATION)\n",
        "    for match in re.finditer(token_regex, expression):\n",
        "        kind = match.lastgroup\n",
        "        value = match.group()\n",
        "        if kind != 'WS':  # Skip whitespace\n",
        "            yield (kind, value)\n",
        "\n",
        "\n",
        "\n",
        "# AST Node Classes\n",
        "class ASTNode: pass\n",
        "\n",
        "class NumberNode(ASTNode):\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "    def __repr__(self):\n",
        "        return f\"NumberNode({self.value})\"\n",
        "\n",
        "class BinOpNode(ASTNode):\n",
        "    def __init__(self, left, op, right):\n",
        "        self.left = left\n",
        "        self.op = op\n",
        "        self.right = right\n",
        "    def __repr__(self):\n",
        "        return f\"BinOpNode({repr(self.left)}, '{self.op}', {repr(self.right)})\"\n",
        "\n",
        "\n",
        "# Operator precedence\n",
        "operator_precedence = {\n",
        "    '+': 15,\n",
        "    '-': 10,\n",
        "    '*': 20,\n",
        "    '/': 30,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parse function\n",
        "def parse(tokens):\n",
        "    tokens = list(tokens)  # Convert generator to list\n",
        "    ast, _ = parse_expr(tokens, 0)\n",
        "    return ast\n",
        "\n",
        "# Parse expressions with precedence handling\n",
        "def parse_expr(tokens, min_precedence):\n",
        "    lhs, tokens = parse_primary(tokens)  # Initial primary expression\n",
        "    while tokens and tokens[0][0] == 'OPERATOR':\n",
        "        op = tokens[0][1]\n",
        "        prec = operator_precedence[op]\n",
        "        if prec >= min_precedence:\n",
        "            # Consume the operator\n",
        "            tokens.pop(0)\n",
        "            # Parse the right-hand side of the operator\n",
        "            rhs, tokens = parse_primary(tokens)\n",
        "            # Recursively call parse_expr to handle the rest of the expression\n",
        "            # This allows handling right-associativity and precedence correctly\n",
        "            while tokens and tokens[0][0] == 'OPERATOR' and operator_precedence[tokens[0][1]] > prec:\n",
        "                rhs, tokens = parse_expr(tokens, operator_precedence[tokens[0][1]])\n",
        "            lhs = BinOpNode(lhs, op, rhs)\n",
        "        else:\n",
        "            break\n",
        "    return lhs, tokens\n",
        "\n",
        "\n",
        "\n",
        "# Parse primary elements (numbers and expressions in parentheses)\n",
        "def parse_primary(tokens):\n",
        "    token = tokens.pop(0)\n",
        "    if token[0] == 'NUMBER':\n",
        "        return NumberNode(token[1]), tokens\n",
        "    elif token[1] == '(':\n",
        "        # Recursively parse the expression inside the parentheses\n",
        "        expr, tokens = parse_expr(tokens, 0)\n",
        "        # Expect and consume the closing parenthesis\n",
        "        if tokens and tokens[0][1] == ')':\n",
        "            tokens.pop(0)  # Remove the ')'\n",
        "            return expr, tokens\n",
        "        else:\n",
        "            raise SyntaxError(\"Expected ')'\")\n",
        "    else:\n",
        "        raise SyntaxError(f\"Unexpected token: {token[1]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Code generation (to Python source code)\n",
        "def generate_python_code(node):\n",
        "    if isinstance(node, NumberNode):\n",
        "        return node.value\n",
        "    elif isinstance(node, BinOpNode):\n",
        "        left_code = generate_python_code(node.left)\n",
        "        right_code = generate_python_code(node.right)\n",
        "        return f\"({left_code} {node.op} {right_code})\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compile expression to Python source code\n",
        "def compile_expression_to_python_source(expression):\n",
        "    tokens = tokenize(expression)\n",
        "    ast = parse(tokens)\n",
        "    python_code = generate_python_code(ast)\n",
        "    return python_code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#outputs\n",
        "#Step 1: Display Tokenization Output\n",
        "def tokenize(expression):\n",
        "    token_regex = '|'.join(f'(?P<{name}>{pattern})' for name, pattern, *_ in TOKEN_SPECIFICATION)\n",
        "    tokens = []\n",
        "    for match in re.finditer(token_regex, expression):\n",
        "        kind = match.lastgroup\n",
        "        value = match.group()\n",
        "        if kind != 'WS':  # Skip whitespace\n",
        "            tokens.append((kind, value))\n",
        "    print(\"Tokens:\", tokens)\n",
        "    return tokens\n",
        "\n",
        "#Step 2: Display Parsing Output (AST Structure)\n",
        "def parse(tokens):\n",
        "    tokens = list(tokens)  # Convert generator to list\n",
        "    ast, _ = parse_expr(tokens, 0)\n",
        "    print(\"AST Structure:\", ast)\n",
        "    return ast\n",
        "\n",
        "#Print\n",
        "expression = \"3 + (5 * 2)-2\"\n",
        "python_code = compile_expression_to_python_source(expression)\n",
        "print(\"Generated Python Source Code:\", python_code)\n",
        "result = eval(python_code)\n",
        "print(\"Result of the expression:\", result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3it4-3Ortt-s",
        "outputId": "86208ceb-df76-414e-e0a7-0fb56e3ac1e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: [('NUMBER', '3'), ('OPERATOR', '+'), ('LPAREN', '('), ('NUMBER', '5'), ('OPERATOR', '*'), ('NUMBER', '2'), ('RPAREN', ')'), ('OPERATOR', '-'), ('NUMBER', '2')]\n",
            "AST Structure: BinOpNode(BinOpNode(NumberNode(3), '+', BinOpNode(NumberNode(5), '*', NumberNode(2))), '-', NumberNode(2))\n",
            "Generated Python Source Code: ((3 + (5 * 2)) - 2)\n",
            "Result of the expression: 11\n"
          ]
        }
      ]
    }
  ]
}